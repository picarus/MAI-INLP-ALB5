README FOR ALB5 INLP TASK
=========================

Table of contents
=================
* Introduction
* API
* NLTK, MODELS AND CORPUS
* Processed dataset
* JSON format
* MySQL database

Introduction
============
This document explains how to use the different outputs generated by the ALB5 module for the INLP project in the MAI 2013-2014.
The ALB5 has two use cases:
1.Use as an API to process individual files or data.
2.Retrieve the whole corpus of non-biographical documents generated by the ALB4 task and (pre-) processed by this module 

API
===
The API is documented in the Python code following Python DOCSTRING guidelines.

NLTK, MODELS AND CORPUS
===============
The NLTK Python library has been used in the implementation of the module. It must be installed in your system for the libraries to work.
The following NLTK corpus must be downloaded to your system, using nltk.download() from the Python IDLE:
* Treebank
Analogously, the following NLTK models must also be installed:
* Punkt


PROCESSED DATASET
=================
For the Non-biographical documents, the output format is a file folder structure, with two folders "Original"(the input) and Processed(the output). In the case of biographical documents, the documents are separated by the occupation they correspond, generating an additional level of folders.
For the Non-biographical documents, each file is expected to be of the format: 
original_<id>.html 
and will be saved with the following format:
processed_<id>.json
where <id> corresponds to the identifier of the file in the provided database. Detailed information is provided in a former section of the document.
For the Biographical documents the structure follows the scheme provided in the occupations.zip in order to read the files.
The generated files are stored in a folder named "clean".
	
 
JSON OUTPUT
===========
A sentence like: “Peter lived in San Francisco.” generates the following output.

{
	"tag":"S",
	"content":
	[
		{
			"tag":"PERSON",
			"content":
			[
				["Peter","NNP","pet"]
			]
		},
		["lived","VBD","liv"],
		["in","IN","in"],
		{
			"tag":"GPE",
			"content":
			[
				["San","NNP","san"],
				["Francisco","NNP","francisco"]
			]
		},
		[".",".","."]
	]
}

A whole text, made up of a sequence of sentences, will be a list of the former structures separated by commas and bracket [] enclosed.

MYSQL DATABASE
==============
A mysql database is provided to ease access to the non-biographical files obtained for each of the characters in the training set. This database has two tables with the following fields.

People
------
id: the id of the person (UNIQUE KEY)
name: the name of the person
profession: the occupation of the person

Article
-------
id: the id of the article (UNIQUE KEY)
link: url of the web page
people_id: id of the people the article belongs to (FOREIGN KEY), corresponds to the People.id field.
is_ bio: indicates if it is a bio or non-bio document
save: indicates that the article content has been validated as relevant to the person.

Other fields are declared that are not used and have no content.

To retrieve all relevant Non-biographical documents:

SELECT * FROM article WHERE save=1 AND is_bio=0

To retrieve all the relevant non-biographical documents for a profession, artist:

SELECT a.id FROM article as a, people as p 
WHERE p.profession="artist" AND a.people_id=p.id AND a.save=1 AND a.is_bio=0

To retrieve the id's of non-biographical and validated documents for a character (Salvador Dali, id=41), the following SQL command must be queried:

SELECT id FROM article WHERE people_id=41 AND save=1 AND is_bio=0 



